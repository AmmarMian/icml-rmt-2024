\section{Proofs of Propositions~\ref{prop:Rgrad_eigsfun} and~\ref{prop:RMT_Fisher_SCM_deterministic_grad}}
\label{app:proofs}

\subsection{Proof of Proposition~\ref{prop:Rgrad_eigsfun}}

Let $f(\point) = g(\eigsMat)$, where we have the eigenvalue decomposition $\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2}=\MAT{U}\eigsMat\MAT{U}^T$.
By definition, we have
$$
    \diff f(\point)[\tangentVector] = \tr(\point^{-1}\grad f(\point)\point^{-1}\tangentVector)
    = \diff g(\eigsMat)[\diff\eigsMat] = \tr(\eigsMat^{-1}\grad g(\eigsMat)\eigsMat^{-1}\diff\eigsMat),
$$
where $\diff\eigsMat=\diag(\MAT{U}^T\diff(\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2})\MAT{U})$.
We further have
$\diff(\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2})=\diff(\point^{\nicefrac{-1}2})\SCM\point^{\nicefrac{-1}2} + \point^{\nicefrac{-1}2}\SCM\diff(\point^{\nicefrac{-1}2})$,
where
$\diff(\point^{\nicefrac{-1}2})\point^{\nicefrac{-1}2} + \point^{\nicefrac{-1}2}\diff(\point^{\nicefrac{-1}2}) = \diff(\point^{-1})=-\point^{-1}\tangentVector\point^{-1}$.
Since $\eigsMat^{-1}\grad g(\eigsMat)\eigsMat^{-1}$ is diagonal, we obtain
$$
     \tr(\eigsMat^{-1}\grad g(\eigsMat)\eigsMat^{-1}\diag(\MAT{U}^T\diff(\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2})\MAT{U}))
     =
     \tr(\eigsMat^{-1}\grad g(\eigsMat)\eigsMat^{-1}\MAT{U}^T\diff(\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2})\MAT{U})
$$
Let $\MAT{M}=\point^{\nicefrac{-1}2}\SCM\point^{\nicefrac{-1}2}$ and $\MAT{D} = \eigsMat^{-1}\grad g(\eigsMat)$.
Since $\MAT{U}\MAT{U}^T=\eye$, we have $\MAT{U}\eigsMat^{-1}\grad g(\eigsMat)\eigsMat^{-1}\MAT{U}^T=\MAT{M}^{-1}\MAT{U}\MAT{D}\MAT{U}^T=\MAT{U}\MAT{D}\MAT{U}^T\MAT{M}^{-1}$ and we obtain
$$
    \diff g(\eigsMat)[\diff\eigsMat] =
    \tr(\MAT{U}\MAT{D}\MAT{U}^T(\diff(\point^{\nicefrac{-1}2})\point^{\nicefrac12} + \point^{\nicefrac12}\diff(\point^{\nicefrac{-1}2})))
$$
Leveraging $\diff(\point^{\nicefrac{-1}2})\point^{\nicefrac{-1}2} + \point^{\nicefrac{-1}2}\diff(\point^{\nicefrac{-1}2}) = -\point^{-1}\tangentVector\point^{-1}$, one can get
$$
    \diff g(\eigsMat)[\diff\eigsMat] = -\tr(\point^{\nicefrac{-1}2}\MAT{U}\MAT{D}\MAT{U}^T\point^{\nicefrac{-1}2}\tangentVector) = \diff f(\point)[\tangentVector] = \tr(\point^{-1}\grad f(\point)\point^{-1}\tangentVector).
$$
The result is finally obtained by identification.



\subsection{Proof of Proposition~\ref{prop:RMT_Fisher_SCM_deterministic_grad}}

To get the gradient of $g$, we first compute its directional derivative $\diff g(\MAT{\Lambda})$ at $\eigsMat$.
Given the eigenvalue decomposition $\eigsMat - \frac{\sqrt{\VEC{\Lambda}}\sqrt{\VEC{\Lambda}^T}}{\nsamples}=\MAT{V}\diag(\VEC{\zeta})\MAT{V}^T$, one can show
\begin{equation*}
    \diff g(\MAT{\Lambda}) = \frac{1}{2p} \diff (\tr(\log^2(\MAT{\Lambda}))) + \frac{1}{p} \diff (\log|\MAT{\Lambda}|) - (\diff \VEC{\lambda}-\diff \VEC{\zeta})^T\left( \frac{1}{p} \MAT{Q}\onevec +\frac{1-c}{c} \VEC{q} \right) - (\VEC{\lambda}-\VEC{\zeta})^T\left( \frac{1}{p} \diff\MAT{Q}\onevec
+\frac{1-c}{c} \diff\VEC{q} \right)
\end{equation*}
where
$\diff\VEC{\zeta}=\diag(\MAT{V}^T\diff(\eigsMat - \frac{\sqrt{\VEC{\Lambda}}\sqrt{\VEC{\Lambda}^T}}{\nsamples})\MAT{V})$;
$\diff\VEC{q} = \diff \MAT{\Lambda} \left( \MAT{I}_p -\log \MAT{\Lambda} \right)\MAT{\Lambda}^{-2}$; 
$\diff \MAT{Q} = \diff \MAT{\Lambda} \MAT{B} + \MAT{C} \diff \MAT{\Lambda}$ with $\MAT{B}$ and $\MAT{C}$ defined in proposition \ref{prop:RMT_Fisher_SCM_deterministic_grad}.
We obtain
$$
    \frac{1}{2p} \diff (\tr(\log^2(\MAT{\Lambda}))) + \frac{1}{p} \diff (\log|\MAT{\Lambda}|)
    =
    \frac1p\tr([\log(\eigsMat)+\eye]\eigsMat^{-1}\diff\eigsMat).
$$
We further have
$$
    - (\diff \VEC{\lambda}-\diff \VEC{\zeta})^T\left( \frac{1}{p} \MAT{Q}\onevec +\frac{1-c}{c} \VEC{q} \right) = -\tr(\MAT{\Delta}\diff\eigsMat) - \tr(\diag(\MAT{A}\MAT{V}\MAT{\Delta}\MAT{A}^T)\diff\eigsMat),
$$
where $\MAT{A}$ and $\MAT{\Delta}$ are defined in proposition \ref{prop:RMT_Fisher_SCM_deterministic_grad}.
We also have
$$
    - \frac{1}{p}(\VEC{\lambda}-\VEC{\zeta})^T\diff\MAT{Q}\onevec
    = -\frac1p\tr(\diag(\MAT{B}\onevec(\VEC{\lambda}-\VEC{\zeta})^T + \onevec(\VEC{\lambda}-\VEC{\zeta})^T\MAT{C})\diff\eigsMat),
$$
and
$$
    -\frac{1-c}{c}(\VEC{\lambda}-\VEC{\zeta})^T\diff\VEC{q}
    = -\frac{1-c}{c}\tr(\eigsMat^{-2}(\eye-\log(\eigsMat))(\eigsMat-\diag(\zeta))\diff\eigsMat).
$$
The result is obtained by combining all above equation and identification with $\tr(\eigsMat^{-2}\grad g(\eigsMat)\diff\eigsMat) = \diff g(\MAT{\Lambda})$.






\section{Simulations for covariance estimation of Section~\ref{sec:cov}}
\label{app:simu_RMTCov}
The experimental setting is as follows: some random covariance $\Cov=\MAT{U}\MAT{\Delta}\MAT{U}^T\in\SPDman$ ($\nfeatures=64$) is generated, where $\MAT{U}$ is uniformly drawn on $\mathcal{O}_{\nfeatures}$ (orthogonal group), and $\MAT{\Delta}$ is randomly drawn on $\DPDman$.
Maximal and minimal diagonal entries of $\MAT{\Delta}$ are set to $\sqrt{a}$ and $\nicefrac{1}{\sqrt{a}}$, where $a=100$ is the condition number.
Remaining non-zero elements are uniformly drawn in-between.
%
From there, matrices $\dataMat\in\realSpace^{\nfeatures\times\nsamples}$ are simulated.
Each column vector of $\dataMat$ is independently drawn from $\mathcal{N}(\VEC{0},\Cov)$.
The effect of the number of samples $\nsamples$ is studied.
We perform $100$ Monte Carlo simulations.

To estimate $\Cov$ from $\dataMat$, we consider the following methods:
(\emph{i}) the SCM estimator $\SCM$;
(\emph{ii}) the linear Ledoit-Wolf estimator $\linearLW$~\cite{ledoit2004well}
(\emph{ii}) the non-linear Ledoit-Wolf estimator $\nonlinearLW$~\cite{ledoit2020analytical}
and (\emph{iii}) our RMT distance based method $\CovRMTdist$ from Algorithm~\ref{algo:RMTCov}.
%
To measure performance, we evaluate the squared Fisher distance~\eqref{eq:Fisher_dist} between $\Cov$ and the estimators.

Results are given in Figure~\ref{fig:cov_simu}.
We observe that the best performance is obtained by $\nonlinearLW$.
Our estimator $\CovRMTdist$ improves upon $\SCM$ and $\linearLW$ at low sample support.
From these results, it does not appear appealing.
It is also computationally significantly more expensive than other estimators, which are analytically known.
%
Thus, exploiting~\eqref{eq:RMT_Fisher_SCM_deterministic} might generally not be suited for covariance estimation.
%
To conclude on a positive note, notice that, while conducting our simulations, we encountered some rare cases at low sample support where $\SCM$, $\linearLW$ and $\nonlinearLW$ behave poorly (especially $\nonlinearLW$), while $\CovRMTdist$ performed well.
We believe that this occurs when the SCM does not provide good eigenvectors.

\begin{figure}[t]
    \centering
    \input{"Figures/mse_estimation_cov/64/plot.tex"}
    \vspace{-2em}
    \caption{MSE of the estimated covariance. Parameters are $p=64$, $\ell_{\mathrm{max}}=100$, $\epsilon=10^{-6}$, $\alpha=10$. Plot done over 1000 trials. The line corresponds to the mean and the filled-are corresponds to the $5$-th and $95$-th quantiles over the trials.}
    \label{fig:cov_simu}
\end{figure}


% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.45\textwidth}
%     \input{"Figures/mse_estimation_cov/5/plot.tex"}
%     \caption{$p=5$}
%     \label{fig:cov est 5}
%   \end{subfigure}
%   \begin{subfigure}{0.45\textwidth}
%     \input{"Figures/mse_estimation_cov/64/plot.tex"}
%     \caption{$p=64$}
%     \label{fig:cov est 64}
%   \end{subfigure}
  
%   \caption{MSE of the estimated covariances towards true matrix for different regimes. For all experiments, the number of maximum iterations of the RMT algorithms is set to $100$ and the stopping criterion is set to $10^{-6}$ and Monte-Carlo has been done over $1000$ trials.}
%   \label{fig:mse nmatrices}
% \end{figure*}

\section{Simulations for mean estimation of Section~\ref{sec:mean}}
\label{app:simu_RMTMean}
We start with the experimental setup.
First, a center $\Mean\in\SPDman$ ($p=64$) is simulated the same way the true covariance $\Cov$ in Section~\ref{sec:cov:simu}.
%
Then, $\nmatrices$ matrices $\{\Cov_k\}$ whose Fréchet mean is $\Mean$ are randomly generated.
To do so, given $k$, we start by drawing $\frac{\nfeatures(\nfeatures+1)}{2}$ values from $\mathcal{N}(0,\sigma^2)$, with $\sigma^2=0.1$.
These are used to canonically construct the symmetric matrix $\MAT{S}_k$.
A set of $\nmatrices$ centered symmetric matrices $\{\tangentVector_k\}$ is obtained by canceling the mean of the $\MAT{S}_k$'s, \emph{i.e.}, $\tangentVector_k = \MAT{S}_k - \frac1{\nmatrices}\sum_{k'}\MAT{S}_{k'}$.
Hence, $\frac1{K}\sum_k\tangentVector_k=\MAT{0}$.
Finally, $\Cov_k=\Mean^{\nicefrac12}\expm(\tangentVector_k)\Mean^{\nicefrac12}$.
%
After that, we generate $\nmatrices$ matrices $\dataMat$ in $\realSpace^{\nfeatures\times\nsamples}$ such that each column of $\dataMat$ is drawn from $\mathcal{N}(\VEC{0},\Cov_k)$.
%
$100$ Monte Carlo runs are performed in order to study the effects of the choices of $\nsamples$ and $\nmatrices$.

To estimate $\Mean$ from $\{\dataMat_k\}$, we consider several methods.
First, we consider two steps methods, which consist in estimating covariance matrices and then their usual Fréchet mean.
The mean resulting from the SCM estimator is denoted $\MeanSCM$.
The ones obtained after employing the linear and non-linear Ledoit-Wolf estimators are denoted $\MeanLinearLW$ and $\MeanNonLinearLW$, respectively.
These are compared to our proposed RMT based mean $\MeanRMT$ obtained with Algorithm~\ref{algo:RMTMean}.
To measure performance, we use the squared Fisher distance~\eqref{eq:Fisher_dist} between the true mean and its estimator.

Results are presented in Figures~\ref{fig:Mean:mse_nsamples} and~\ref{fig:Mean:mse_nmatrices}.
Figure~\ref{fig:Mean:mse_nsamples} illustrates the effect of varying the number of samples $\nsamples$ while the number of matrices $\nmatrices$ is fixed.
Figure~\ref{fig:Mean:mse_nmatrices} shows the effect of the choice of $\nmatrices$ while $\nsamples$ is fixed.
%
We clearly observe that our proposed RMT based mean estimator $\MeanRMT$ outperforms other methods.
In both cases, $\MeanLinearLW$ features very poor performance.
When $\nsamples$ increases, $\MeanSCM$ and $\MeanNonLinearLW$ slowly catch up with $\MeanRMT$.
However, when $\nsamples$ is fixed (low support, but not that much), as $\nmatrices$ grows, the performance of $\MeanSCM$ and $\MeanNonLinearLW$ reach a plateau while the one of $\MeanRMT$ strongly improves.
%
In conclusion, when the available amount of samples $\nsamples$ is somewhat limited, our proposed RMT based method is very advantageous as compared to the others, especially if $\nmatrices$ is large.

\begin{figure}
    \centering
    \input{"Figures/mse_nsamples/plot.tex"}
    \vspace{-2em}
    \caption{MSE of the estimated Fréchet-mean towards true mean matrix. Parameters are $p=64$, $K=10$, $\ell_{\mathrm{max}}=100$, $\epsilon=10^{-6}$, $\alpha = 10$. Plot done over 1000 trials. The line corresponds to the mean and the filled-are corresponds to the $5$-th and $95$-th quantiles over the trials.}
    \label{fig:Mean:mse_nsamples}
\end{figure}

\begin{figure}
    \centering
    \input{"Figures/mse_nmatrices/64_128/plot_legend"}
    \vspace{-2em}
    \caption{MSE of the estimated Fréchet-mean towards true mean matrix. Parameters are $p=64$, $n=128$, $\ell_{\mathrm{max}}=100$, $\epsilon=10^{-6}$, $\alpha = 10$. Plot done over 100 trials. The line corresponds to the mean and the filled-are corresponds to the $5$-th and $95$-th quantiles over the trials.}
    \label{fig:Mean:mse_nmatrices}
\end{figure}


% \begin{figure}[h]
%     \centering
%     \input{Figures/mse_iteration/mse_iteration}
%     \caption{MSE of the estimated mean towards true mean versus the algorithms' iterations. No value after some iteration means that stopping criterion has been attained. Monte-carlo has been done with 10000 trials. Parameters are $p=5$, $n=49$ and $K=100$.}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}{0.4\textwidth}
%     \input{"Figures/mse_nmatrices/5_7/output.tex"}
%     \caption{$p=5$, $n=7$}
%     \label{fig:subfiga}
%   \end{subfigure}
%   \begin{subfigure}{0.4\textwidth}
%     \input{"Figures/mse_nmatrices/5_25/output.tex"}
%     \caption{$p=5$, $n=25$}
%     \label{fig:subfigb}
%   \end{subfigure}
%   \begin{subfigure}{0.4\textwidth}
%     \input{"Figures/mse_nmatrices/64_66/output.tex"}
%     \caption{$p=64$, $n=66$}
%     \label{fig:subfiga}
%   \end{subfigure}
%   \begin{subfigure}{0.4\textwidth}
%     \input{"Figures/mse_nmatrices/64_128/output.tex"}
%     \caption{$p=64$, $n=128$}
%     \label{fig:subfigb}
%   \end{subfigure}
%   \begin{subfigure}{0.4\textwidth}
%     \input{"Figures/mse_nmatrices/64_512/output.tex"}
%     \caption{$p=64$, $n=512$}
%     \label{fig:subfigb}
%   \end{subfigure}
  
%   \caption{MSE of the estimated mean towards true mean for different regimes. For all experiments, the number of maximum iterations of the RMT algorithms is set to $100$ and the stopping criterion is set to $10^{-6}$ and Monte-Carlo has been done over $1000$ trials for $p=5$ and $100$ for $p=64$.}
%   \label{fig:mse nmatrices}
% \end{figure*}



